# Attention and Transformers

Attention is an added layer that lets a model focus on what is important. `Queries`, `Values` and `Keys` are used for information retrieval inside the Attention layer.

The attention mechanism uses encoded representations of both the input (encoder hidden states) and the outputs (decoder hidden states)

## Resources

* [Attention is All you need](https://arxiv.org/pdf/1706.03762.pdf)
* [NLP from scratch: Translation with a Seq2Seq Network and Attention](https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html#training-the-model)
* [Exploring the limits of Transfer Learning with a Unified Text to Text Transformer](https://arxiv.org/abs/1910.10683)
* [Reformer: The Efficient Transformer](https://arxiv.org/abs/2001.04451)
* [Deep contextualized word representations](https://arxiv.org/pdf/1802.05365.pdf)
* [The Illustrated Transformer](http://jalammar.github.io/illustrated-transformer/)
* [The Illustrated GPT-2](http://jalammar.github.io/illustrated-gpt2/)
* [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/abs/1810.04805)
* [How GPT3 Works - Visualizations and Animations](http://jalammar.github.io/how-gpt3-works-visualizations-animations/)
